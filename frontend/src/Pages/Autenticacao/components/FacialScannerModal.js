import React, { useState, useRef, useEffect } from 'react';
import { View, Text, TouchableOpacity, Modal, StyleSheet } from 'react-native';
import * as faceapi from 'face-api.js';

const FacialScannerModal = ({ visible, onClose, onScanComplete, t }) => {
    const [isScanning, setIsScanning] = useState(false);
    const [scanProgress, setScanProgress] = useState(0);
    const [statusMessage, setStatusMessage] = useState('');
    const [cameraReady, setCameraReady] = useState(false);
    const [modelsLoaded, setModelsLoaded] = useState(false);
    const videoRef = useRef(null);
    const canvasRef = useRef(null);
    const streamRef = useRef(null);

    useEffect(() => {
        if (visible) {
            initializeFaceAPI();
        } else {
            stopCamera();
            setStatusMessage('');
        }

        return () => {
            stopCamera();
        };
    }, [visible]);

    const initializeFaceAPI = async () => {
        try {
            setStatusMessage('Carregando modelos de detec√ß√£o facial...');

            // Usar CDN diretamente para maior confiabilidade
            const CDN_MODEL_URL = 'https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights';

            // Carregar apenas os modelos essenciais
            await Promise.all([
                faceapi.nets.tinyFaceDetector.loadFromUri(CDN_MODEL_URL),
                faceapi.nets.faceLandmark68Net.loadFromUri(CDN_MODEL_URL),
                faceapi.nets.faceRecognitionNet.loadFromUri(CDN_MODEL_URL)
            ]);

            setModelsLoaded(true);
            setStatusMessage('Modelos carregados! Iniciando c√¢mera...');
            await startCamera();

        } catch (error) {
            console.error('Erro ao carregar modelos face-api.js:', error);
            setStatusMessage('Erro ao carregar modelos de detec√ß√£o facial.');
            
            // Tentar carregar apenas o detector b√°sico como fallback
            try {
                setStatusMessage('Tentando modo b√°sico de detec√ß√£o...');
                
                await faceapi.nets.tinyFaceDetector.loadFromUri(CDN_MODEL_URL);
                
                setModelsLoaded(true);
                setStatusMessage('Modo b√°sico carregado! Iniciando c√¢mera...');
                await startCamera();
                
            } catch (fallbackError) {
                console.error('Erro no fallback:', fallbackError);
                setStatusMessage('N√£o foi poss√≠vel carregar os modelos de detec√ß√£o facial. Verifique sua conex√£o.');
            }
        }
    };

    const startCamera = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                video: {
                    width: { ideal: 640 },
                    height: { ideal: 480 },
                    facingMode: 'user'
                }
            });

            if (videoRef.current) {
                videoRef.current.srcObject = stream;
                streamRef.current = stream;

                videoRef.current.onloadedmetadata = () => {
                    setCameraReady(true);
                    setStatusMessage('Posicione o rosto dentro do c√≠rculo.');
                    // Start drawing detections once metadata is loaded and models are ready
                    if (modelsLoaded) {
                        detectFacesOnStream();
                    }
                };
            }
        } catch (error) {
            console.error('Erro ao acessar c√¢mera:', error);
            setCameraReady(false);
            setStatusMessage('Erro ao acessar a c√¢mera. Verifique as permiss√µes.');
            alert('Erro ao acessar a c√¢mera. Por favor, conceda as permiss√µes necess√°rias.');
        }
    };

    const stopCamera = () => {
        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
            setCameraReady(false);
            // Clear canvas drawings
            const canvas = canvasRef.current;
            if (canvas) {
                const ctx = canvas.getContext('2d');
                ctx.clearRect(0, 0, canvas.width, canvas.height);
            }
        }
    };

    const detectFacesOnStream = async () => {
        const video = videoRef.current;
        const canvas = canvasRef.current;
        if (!video || !canvas || !modelsLoaded) return;

        try {
            const displaySize = { width: video.videoWidth || 640, height: video.videoHeight || 480 };
            faceapi.matchDimensions(canvas, displaySize);

            // Detectar faces com fallbacks progressivos
            let detections;
            try {
                // Tentar com landmarks e reconhecimento
                detections = await faceapi
                    .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceDescriptors();
            } catch (detectionError) {
                try {
                    // Fallback para detec√ß√£o com landmarks apenas
                    detections = await faceapi
                        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                        .withFaceLandmarks();
                } catch (landmarkError) {
                    // Fallback final para detec√ß√£o b√°sica
                    detections = await faceapi
                        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
                }
            }

            const resizedDetections = faceapi.resizeResults(detections, displaySize);

            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            // Draw bounding boxes and landmarks
            resizedDetections.forEach(detection => {
                const box = detection.detection.box;
                const drawBox = new faceapi.draw.DrawBox(box, {
                    label: `Confian√ßa: ${Math.round(detection.detection.score * 100)}%`,
                    boxColor: '#1792FE'
                });
                drawBox.draw(canvas);

                // S√≥ desenhar landmarks se estiverem dispon√≠veis
                if (detection.landmarks) {
                    const drawLandmarks = new faceapi.draw.DrawLandmarks(detection.landmarks, {
                        drawLines: true,
                        color: '#FFEB3B'
                    });
                    drawLandmarks.draw(canvas);
                }
            });

            // Continue the detection loop
            requestAnimationFrame(detectFacesOnStream);
            
        } catch (error) {
            console.error('Erro na detec√ß√£o cont√≠nua:', error);
            // Tentar novamente ap√≥s um pequeno delay
            setTimeout(detectFacesOnStream, 1000);
        }
    };

    const startScan = async () => {
        if (!cameraReady || !modelsLoaded) {
            setStatusMessage('Sistema ainda n√£o est√° pronto. Aguarde o carregamento.');
            return;
        }

        // Perform a single detection to check readiness for scan
        const video = videoRef.current;
        if (!video) return;

        let detections;
        try {
            detections = await faceapi
                .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                .withFaceLandmarks()
                .withFaceDescriptors();
        } catch (error) {
            try {
                detections = await faceapi
                    .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks();
            } catch (landmarkError) {
                detections = await faceapi
                    .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
            }
        }

        if (detections.length === 0) {
            setStatusMessage('Face n√£o detectada. Posicione-se corretamente.');
            return;
        }
        if (detections.length > 1) {
            setStatusMessage('M√∫ltiplas faces detectadas. Apenas uma pessoa permitida.');
            return;
        }

        const detection = detections[0];
        const confidence = detection.detection.score;
        const qualityScore = calculateFaceQuality(detection, video);

        if (confidence < 0.6 || qualityScore < 0.4) {
            setStatusMessage(`Qualidade insuficiente (Confian√ßa: ${Math.round(confidence * 100)}%, Qualidade: ${Math.round(qualityScore * 100)}%). Ajuste seu posicionamento.`);
            return;
        }

        setIsScanning(true);
        setScanProgress(0);
        setStatusMessage('Face detectada! Iniciando captura biom√©trica...');

        const scans = [];
        const totalScans = 8;
        let currentScan = 0;
        let failedScans = 0;

        const interval = setInterval(async () => {
            let scanDetection;
            try {
                scanDetection = await faceapi
                    .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceDescriptors();
            } catch (error) {
                try {
                    scanDetection = await faceapi
                        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                        .withFaceLandmarks();
                } catch (landmarkError) {
                    scanDetection = await faceapi
                        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
                }
            }

            if (scanDetection.length > 0) {
                const currentDetection = scanDetection[0]; // Assume a primeira face detectada
                const currentConfidence = currentDetection.detection.score;
                const currentQuality = calculateFaceQuality(currentDetection, video);

                if (currentConfidence > 0.6 && currentQuality > 0.4) {
                    scans.push({
                        confidence: currentConfidence,
                        qualityScore: currentQuality,
                        landmarks: currentDetection.landmarks,
                        descriptor: currentDetection.descriptor,
                        expressions: currentDetection.expressions // Include expressions if needed
                    });
                    currentScan++;

                    const progress = (currentScan / totalScans) * 100;
                    setScanProgress(progress);

                    if (progress < 25) {
                        setStatusMessage(`Capturando dados biom√©tricos ${currentScan}/${totalScans} (Confian√ßa: ${Math.round(currentConfidence * 100)}%)`);
                    } else if (progress < 50) {
                        setStatusMessage(`Analisando caracter√≠sticas faciais... ${currentScan}/${totalScans}`);
                    } else if (progress < 75) {
                        setStatusMessage(`Gerando template biom√©trico... ${currentScan}/${totalScans}`);
                    } else {
                        setStatusMessage(`Finalizando captura... ${currentScan}/${totalScans}`);
                    }

                    if (currentScan >= totalScans) {
                        clearInterval(interval);
                        completeScanWithFaceAPI(scans);
                    }
                } else {
                    failedScans++;
                    setStatusMessage(`Ajustando captura... (Confian√ßa: ${Math.round(currentConfidence * 100)}%)`);
                }
            } else {
                failedScans++;
                setStatusMessage('Face n√£o detectada nesta amostra. Ajustando...');
            }

            if (failedScans > 3) {
                clearInterval(interval);
                setIsScanning(false);
                setScanProgress(0);
                setStatusMessage('Qualidade da captura insuficiente. Tente novamente.');
            }
        }, 500);
    };

    const calculateFaceQuality = (detection, video) => {
        const { width, height } = video;
        const box = detection.detection.box;

        // Center face - mais tolerante
        const faceCenterX = box.x + box.width / 2;
        const faceCenterY = box.y + box.height / 2;
        const imageCenterX = width / 2;
        const imageCenterY = height / 2;
        const centerDistance = Math.sqrt(Math.pow(faceCenterX - imageCenterX, 2) + Math.pow(faceCenterY - imageCenterY, 2));
        const maxDistance = Math.sqrt(Math.pow(width / 2, 2) + Math.pow(height / 2, 2));
        const centeringScore = Math.max(0.3, 1 - (centerDistance / maxDistance)); // M√≠nimo de 0.3

        // Face size - muito mais tolerante
        const faceArea = box.width * box.height;
        const videoArea = width * height;
        const faceRatio = faceArea / videoArea;
        let sizeScore = 0;
        if (faceRatio > 0.02 && faceRatio < 0.8) sizeScore = 1.0; // Muito mais tolerante
        else if (faceRatio > 0.01 && faceRatio < 0.9) sizeScore = 0.8;
        else sizeScore = 0.5; // Ainda assim dar alguma pontua√ß√£o

        // Landmark detection quality - mais tolerante
        const landmarkQuality = detection.landmarks ? 1.0 : 0.7; // Mesmo sem landmarks, dar 0.7

        // Dar mais peso √† confian√ßa da detec√ß√£o
        const confidenceScore = detection.detection.score;

        return Math.max(0.4, (centeringScore * 0.2 + sizeScore * 0.3 + landmarkQuality * 0.2 + confidenceScore * 0.3));
    };

    const completeScanWithFaceAPI = async (scans) => {
        try {
            const avgMetrics = calculateAverageDetections(scans);
            const overallConfidence = avgMetrics.avgConfidence;

            if (overallConfidence < 0.75) {
                setIsScanning(false);
                setScanProgress(0);
                setStatusMessage(`Confian√ßa insuficiente (${Math.round(overallConfidence * 100)}%). Tente novamente.`);
                return;
            }

            setStatusMessage(`Processando template biom√©trico... (Confian√ßa: ${Math.round(overallConfidence * 100)}%)`);

            const canvas = canvasRef.current;
            const video = videoRef.current;

            if (canvas && video) {
                // Capture final image for processing
                const ctx = canvas.getContext('2d');
                ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear previous drawings
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageDataUrl = canvas.toDataURL('image/jpeg', 0.9);

                const facialData = await processFacialBiometricsWithAPI(imageDataUrl, avgMetrics, overallConfidence);

                setIsScanning(false);
                setScanProgress(0);
                setStatusMessage(`Captura conclu√≠da! Confian√ßa: ${Math.round(overallConfidence * 100)}%`);

                setTimeout(() => {
                    onScanComplete(facialData);
                    onClose();
                }, 2000);
            }
        } catch (error) {
            console.error('Erro ao completar scan:', error);
            setIsScanning(false);
            setScanProgress(0);
            setStatusMessage('Erro ao processar dados faciais.');
            alert('Ocorreu um erro ao processar seus dados faciais. Por favor, tente novamente.');
        }
    };

    const calculateAverageDetections = (scans) => {
        if (scans.length === 0) return null;

        let totalConfidence = 0;
        let totalQuality = 0;
        const allDescriptors = [];
        const allLandmarks = [];

        scans.forEach(scan => {
            totalConfidence += scan.confidence;
            totalQuality += scan.qualityScore;
            if (scan.descriptor) allDescriptors.push(scan.descriptor);
            if (scan.landmarks) allLandmarks.push(scan.landmarks);
        });

        const count = scans.length;
        const avgDescriptor = calculateAverageDescriptor(allDescriptors);
        const avgLandmarks = calculateAverageLandmarks(allLandmarks);

        return {
            avgConfidence: totalConfidence / count,
            avgQuality: totalQuality / count,
            sampleCount: count,
            consistency: calculateDescriptorConsistency(allDescriptors),
            avgDescriptor: avgDescriptor,
            avgLandmarks: avgLandmarks
        };
    };

    const calculateAverageDescriptor = (descriptors) => {
        if (descriptors.length === 0) return null;
        const descriptorLength = descriptors[0].length;
        const avgDescriptor = new Array(descriptorLength).fill(0);

        descriptors.forEach(descriptor => {
            descriptor.forEach((value, index) => {
                avgDescriptor[index] += value;
            });
        });

        return avgDescriptor.map(sum => sum / descriptors.length);
    };

    const calculateAverageLandmarks = (landmarksArray) => {
        if (landmarksArray.length === 0) return null;

        // Assumindo que todos os landmarks t√™m a mesma estrutura (ex: 68 pontos)
        const landmarkCount = landmarksArray[0].positions.length;
        const avgLandmarks = Array.from({ length: landmarkCount }, () => ({ x: 0, y: 0 }));

        landmarksArray.forEach(landmarks => {
            landmarks.positions.forEach((pos, index) => {
                avgLandmarks[index].x += pos.x;
                avgLandmarks[index].y += pos.y;
            });
        });

        return {
            positions: avgLandmarks.map(p => ({ x: p.x / landmarksArray.length, y: p.y / landmarksArray.length }))
        };
    };

    const calculateDescriptorConsistency = (descriptors) => {
        if (descriptors.length < 2) return 1.0;

        const avgDescriptor = calculateAverageDescriptor(descriptors);
        let totalVariance = 0;

        descriptors.forEach(descriptor => {
            let variance = 0;
            descriptor.forEach((value, index) => {
                variance += Math.pow(value - avgDescriptor[index], 2);
            });
            totalVariance += variance / descriptor.length;
        });

        const avgVariance = totalVariance / descriptors.length;
        // Normalize variance to a consistency score (0 to 1)
        // The scaling factor (e.g., 10) might need tuning based on typical variance values
        return Math.max(0, Math.min(1, 1 - (avgVariance / 10)));
    };

    const processFacialBiometricsWithAPI = async (imageDataUrl, avgMetrics, overallConfidence) => {
        const img = new Image();
        img.src = imageDataUrl;

        return new Promise((resolve, reject) => {
            img.onload = () => {
                const features = {
                    faceDetected: true,
                    confidence: overallConfidence,
                    qualityScore: avgMetrics.avgQuality,
                    algorithm: 'face-api.js',
                    version: '1.0.0',
                    biometricTemplate: {
                        descriptor: avgMetrics.avgDescriptor,
                        landmarks: avgMetrics.avgLandmarks,
                        consistency: avgMetrics.consistency,
                        sampleCount: avgMetrics.sampleCount
                    },
                    scanQuality: {
                        confidence: overallConfidence,
                        qualityScore: avgMetrics.avgQuality,
                        consistency: avgMetrics.consistency,
                        sampleCount: avgMetrics.sampleCount,
                        securityLevel: avgMetrics.consistency > 0.8 ? 'HIGH' : avgMetrics.consistency > 0.6 ? 'MEDIUM' : 'LOW'
                    },
                    timestamp: new Date().toISOString(),
                    validationPassed: true,
                    captureMethod: 'face-api.js',
                    encoding: avgMetrics.avgDescriptor,
                    landmarks: convertLandmarksToLegacyFormat(avgMetrics.avgLandmarks)
                };
                resolve({
                    type: 'facial',
                    data: JSON.stringify(features),
                    imageData: imageDataUrl.includes(',') ? imageDataUrl.split(',')[1] : imageDataUrl
                });
            };
            img.onerror = () => reject(new Error('Erro ao processar imagem'));
        });
    };

    const convertLandmarksToLegacyFormat = (landmarks) => {
        if (!landmarks || !landmarks.positions || landmarks.positions.length === 0) {
            const video = videoRef.current;
            if (!video) return null;
            const centerX = video.videoWidth / 2;
            const centerY = video.videoHeight / 2;
            return {
                eyes: { left: [centerX - video.videoWidth * 0.1, centerY - video.videoHeight * 0.1], right: [centerX + video.videoWidth * 0.1, centerY - video.videoHeight * 0.1] },
                nose: [centerX, centerY],
                mouth: [centerX, centerY + video.videoHeight * 0.1]
            };
        }

        const positions = landmarks.positions;
        return {
            eyes: {
                left: [positions[36].x, positions[36].y], // Canto externo olho esquerdo
                right: [positions[45].x, positions[45].y] // Canto externo olho direito
            },
            nose: [positions[30].x, positions[30].y], // Ponta do nariz
            mouth: [positions[51].x, positions[51].y] // Centro l√°bio inferior
        };
    };

    // Estilos ajustados para renderiza√ß√£o em ambiente que suporta CSS (como Web)
    // Para React Native puro, estes seriam convertidos via StyleSheet.create
    const styles = {
        modalOverlay: {
            position: 'fixed', top: 0, left: 0, width: '100%', height: '100%',
            backgroundColor: 'rgba(0, 0, 0, 0.8)', display: 'flex',
            justifyContent: 'center', alignItems: 'center', zIndex: 1000,
        },
        modalContainer: {
            backgroundColor: '#1a1a1a', borderRadius: 15, width: '90%', maxWidth: '500px',
            overflow: 'hidden', boxShadow: '0 5px 15px rgba(0,0,0,0.5)',
            display: 'flex', flexDirection: 'column',
        },
        header: {
            backgroundColor: '#1792FE', padding: '15px 20px', display: 'flex',
            justifyContent: 'space-between', alignItems: 'center',
        },
        title: {
            fontSize: '24px', fontWeight: 'bold', color: 'white', margin: 0,
        },
        closeButton: {
            backgroundColor: 'transparent', border: 'none', color: 'white',
            fontSize: '28px', cursor: 'pointer', fontWeight: 'bold',
        },
        cameraContainer: {
            position: 'relative', width: '100%', height: '350px',
            display: 'flex', justifyContent: 'center', alignItems: 'center',
            backgroundColor: '#000',
        },
        video: {
            display: 'block', width: '100%', height: '100%', objectFit: 'cover',
        },
        overlay: {
            position: 'absolute', top: 0, left: 0, right: 0, bottom: 0,
            display: 'flex', justifyContent: 'center', alignItems: 'center',
            flexDirection: 'column',
        },
        faceGuide: {
            width: '80%', maxWidth: '200px', height: '200px', borderRadius: '100px',
            border: '3px dashed #1792FE', display: 'flex', justifyContent: 'center',
            alignItems: 'center', backgroundColor: 'rgba(23, 146, 254, 0.1)',
            position: 'relative',
        },
        faceCircle: {
            width: '100%', height: '100%', borderRadius: '100%',
            border: '2px solid rgba(255, 255, 255, 0.5)',
        },
        landmarks: { // Container for drawing landmarks on canvas
            position: 'absolute', top: 0, left: 0, width: '100%', height: '100%',
            pointerEvents: 'none', // Allow interaction with elements below
        },
        // Landmark points are drawn directly on the canvas by face-api.js
        statusContainer: {
            padding: '15px 20px', backgroundColor: 'rgba(0, 0, 0, 0.8)',
            textAlign: 'center',
        },
        statusMessage: {
            color: 'white', fontSize: '16px', marginBottom: '10px', minHeight: '20px',
        },
        progressContainer: {
            display: 'flex', alignItems: 'center', justifyContent: 'center', marginTop: '10px',
        },
        progressBar: {
            flexGrow: 1, height: '8px', backgroundColor: 'rgba(255, 255, 255, 0.3)',
            borderRadius: '4px', marginRight: '10px', overflow: 'hidden',
        },
        progressFill: {
            height: '100%', backgroundColor: '#1792FE', borderRadius: '4px',
            transition: 'width 0.2s ease-in-out',
        },
        progressText: {
            color: 'white', fontSize: '14px', fontWeight: 'bold',
        },
        controls: {
            padding: '20px', backgroundColor: 'rgba(0, 0, 0, 0.8)',
            display: 'flex', justifyContent: 'center',
        },
        scanButton: {
            backgroundColor: '#1792FE', padding: '15px 25px', borderRadius: '12px',
            border: 'none', color: 'white', fontSize: '18px', fontWeight: 'bold',
            cursor: 'pointer', transition: 'background-color 0.3s ease',
            display: 'flex', alignItems: 'center', gap: '10px',
        },
        scanButtonDisabled: {
            backgroundColor: '#666', cursor: 'not-allowed',
        },
        instructions: {
            padding: '20px', backgroundColor: 'rgba(0, 0, 0, 0.8)',
            textAlign: 'center', borderTop: '1px solid #333',
        },
        instructionItem: {
            color: 'white', fontSize: '14px', marginBottom: '8px',
            display: 'flex', alignItems: 'center', justifyContent: 'center', gap: '8px',
        },
    };

    // If not visible, return null
    if (!visible) return null;

    return (
        <div style={styles.modalOverlay}>
            <div style={styles.modalContainer}>
                <div style={styles.header}>
                    <h2 style={styles.title}>Scanner Facial</h2>
                    <button onClick={onClose} style={styles.closeButton}>
                        ‚úï
                    </button>
                </div>

                <div style={styles.cameraContainer}>
                    <video
                        ref={videoRef}
                        style={styles.video}
                        autoPlay
                        playsInline
                        muted
                        // We will draw detections on a canvas overlay
                    />
                    <canvas
                        ref={canvasRef}
                        style={{ position: 'absolute', top: 0, left: 0, width: '100%', height: '100%', pointerEvents: 'none' }}
                    />

                    <div style={styles.overlay}>
                        <div style={styles.faceGuide}>
                            <div style={styles.faceCircle}></div>
                        </div>
                        {/* Landmarks will be drawn directly on the canvas by face-api */}
                    </div>
                </div>

                <div style={styles.statusContainer}>
                    <div style={styles.statusMessage}>
                        {statusMessage}
                    </div>

                    {isScanning && (
                        <div style={styles.progressContainer}>
                            <div style={styles.progressBar}>
                                <div
                                    style={{
                                        ...styles.progressFill,
                                        width: `${scanProgress}%`
                                    }}
                                />
                            </div>
                            <span style={styles.progressText}>{Math.round(scanProgress)}%</span>
                        </div>
                    )}
                </div>

                {!isScanning && cameraReady && modelsLoaded && (
                    <div style={styles.controls}>
                        <button
                            style={styles.scanButton}
                            onClick={startScan}
                        >
                            üîç Iniciar Scan Biom√©trico
                        </button>
                    </div>
                )}

                <div style={styles.instructions}>
                    <div style={styles.instructionItem}>
                        ü§ñ Sistema de detec√ß√£o facial avan√ßado (face-api.js)
                    </div>
                    <div style={styles.instructionItem}>
                        üí° Mantenha o rosto bem iluminado e centralizado
                    </div>
                    <div style={styles.instructionItem}>
                        üëÄ Olhe diretamente para a c√¢mera
                    </div>
                    <div style={styles.instructionItem}>
                        üéØ Apenas uma pessoa deve estar vis√≠vel
                    </div>
                </div>
            </div>
        </div>
    );
};

// Note: The StyleSheet.create part from the original code is removed
// as this implementation assumes a web-like environment where inline styles or CSS files are used.
// For a true React Native environment, you would need to manage styles differently
// and potentially use a library that bridges face-api.js with React Native's camera/canvas components.

export default FacialScannerModal;